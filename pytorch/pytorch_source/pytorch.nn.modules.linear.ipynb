{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a447c4-ab99-4fbb-b177-60dfd1d2dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from .. import functional as F\n",
    "from .. import init\n",
    "from .module import Module\n",
    "from .lazy import LazyModuleMixin\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'Bilinear',\n",
    "    'Identity',\n",
    "    'LazyLinear',\n",
    "    'Linear',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "[docs]class Identity(Module):\n",
    "    r\"\"\"A placeholder identity operator that is argument-insensitive.\n",
    "\n",
    "    Args:\n",
    "        args: any argument (unused)\n",
    "        kwargs: any keyword argument (unused)\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
    "        - Output: :math:`(*)`, same shape as the input.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 20])\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[docs]class Linear(Module):\n",
    "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
    "\n",
    "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
    "\n",
    "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
    "          dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
    "        - Output: :math:`(*, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
    "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
    "                If :attr:`bias` is ``True``, the values are initialized from\n",
    "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
    "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Linear(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n",
    "        # https://github.com/pytorch/pytorch/issues/57109\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'in_features={self.in_features}, out_features={self.out_features}, bias={self.bias is not None}'\n",
    "\n",
    "\n",
    "\n",
    "# This class exists solely to avoid triggering an obscure error when scripting\n",
    "# an improperly quantized attention layer. See this issue for details:\n",
    "# https://github.com/pytorch/pytorch/issues/58969\n",
    "# TODO: fail fast on quantization API usage error, then remove this class\n",
    "# and replace uses of it with plain Linear\n",
    "class NonDynamicallyQuantizableLinear(Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        super().__init__(in_features, out_features, bias=bias,\n",
    "                         device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "\n",
    "[docs]class Bilinear(Module):\n",
    "    r\"\"\"Applies a bilinear transformation to the incoming data: :math:`y = x_1^T A x_2 + b`.\n",
    "\n",
    "    Args:\n",
    "        in1_features: size of each first input sample\n",
    "        in2_features: size of each second input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to False, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input1: :math:`(*, H_{in1})` where :math:`H_{in1}=\\text{in1\\_features}` and\n",
    "          :math:`*` means any number of additional dimensions including none. All but the last dimension\n",
    "          of the inputs should be the same.\n",
    "        - Input2: :math:`(*, H_{in2})` where :math:`H_{in2}=\\text{in2\\_features}`.\n",
    "        - Output: :math:`(*, H_{out})` where :math:`H_{out}=\\text{out\\_features}`\n",
    "          and all but the last dimension are the same shape as the input.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            :math:`(\\text{out\\_features}, \\text{in1\\_features}, \\text{in2\\_features})`.\n",
    "            The values are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "            :math:`k = \\frac{1}{\\text{in1\\_features}}`\n",
    "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
    "                If :attr:`bias` is ``True``, the values are initialized from\n",
    "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "                :math:`k = \\frac{1}{\\text{in1\\_features}}`\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Bilinear(20, 30, 40)\n",
    "        >>> input1 = torch.randn(128, 20)\n",
    "        >>> input2 = torch.randn(128, 30)\n",
    "        >>> output = m(input1, input2)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 40])\n",
    "    \"\"\"\n",
    "\n",
    "    __constants__ = ['in1_features', 'in2_features', 'out_features']\n",
    "    in1_features: int\n",
    "    in2_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in1_features: int, in2_features: int, out_features: int, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.in1_features = in1_features\n",
    "        self.in2_features = in2_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.empty((out_features, in1_features, in2_features), **factory_kwargs))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        bound = 1 / math.sqrt(self.weight.size(1))\n",
    "        init.uniform_(self.weight, -bound, bound)\n",
    "        if self.bias is not None:\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input1: Tensor, input2: Tensor) -> Tensor:\n",
    "        return F.bilinear(input1, input2, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in1_features={}, in2_features={}, out_features={}, bias={}'.format(\n",
    "            self.in1_features, self.in2_features, self.out_features, self.bias is not None\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[docs]class LazyLinear(LazyModuleMixin, Linear):\n",
    "    r\"\"\"A :class:`torch.nn.Linear` module where `in_features` is inferred.\n",
    "\n",
    "    In this module, the `weight` and `bias` are of :class:`torch.nn.UninitializedParameter`\n",
    "    class. They will be initialized after the first call to ``forward`` is done and the\n",
    "    module will become a regular :class:`torch.nn.Linear` module. The ``in_features`` argument\n",
    "    of the :class:`Linear` is inferred from the ``input.shape[-1]``.\n",
    "\n",
    "    Check the :class:`torch.nn.modules.lazy.LazyModuleMixin` for further documentation\n",
    "    on lazy modules and their limitations.\n",
    "\n",
    "    Args:\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
    "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
    "                If :attr:`bias` is ``True``, the values are initialized from\n",
    "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
    "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    cls_to_become = Linear  # type: ignore[assignment]\n",
    "    weight: UninitializedParameter\n",
    "    bias: UninitializedParameter  # type: ignore[assignment]\n",
    "\n",
    "    def __init__(self, out_features: int, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        # bias is hardcoded to False to avoid creating tensor\n",
    "        # that will soon be overwritten.\n",
    "        super().__init__(0, 0, False)\n",
    "        self.weight = UninitializedParameter(**factory_kwargs)\n",
    "        self.out_features = out_features\n",
    "        if bias:\n",
    "            self.bias = UninitializedParameter(**factory_kwargs)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        if not self.has_uninitialized_params() and self.in_features != 0:\n",
    "            super().reset_parameters()\n",
    "\n",
    "    def initialize_parameters(self, input) -> None:  # type: ignore[override]\n",
    "        if self.has_uninitialized_params():\n",
    "            with torch.no_grad():\n",
    "                self.in_features = input.shape[-1]\n",
    "                self.weight.materialize((self.out_features, self.in_features))\n",
    "                if self.bias is not None:\n",
    "                    self.bias.materialize((self.out_features,))\n",
    "                self.reset_parameters()\n",
    "\n",
    "# TODO: PartialLinear - maybe in sparse?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
